{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ROCKET.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIYhGppnXag0",
        "outputId": "120ccd78-07b5-4ac4-a6da-7383abc9894f"
      },
      "source": [
        "env = 'colab' if 'google.colab' in str(get_ipython()) else 'local'\n",
        "\n",
        "if env == 'colab':\n",
        "    print('Running on CoLab')\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    import sys\n",
        "    FOLDERNAME = 'Learning/ATS-Yandex/ROCKET/'\n",
        "    sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "    \n",
        "    %cd drive/My\\ Drive/$FOLDERNAME/\n",
        "    input_path = r\"/content/drive/My Drive/Learning/ATS-Yandex/ROCKET/data\"\n",
        "else:\n",
        "    print('Running locally')\n",
        "    input_path = 'data'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CoLab\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Learning/ATS-Yandex/ROCKET\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JDLA-Hg-rAq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader \n",
        "import torch.nn as nn\n",
        "from torch import optim, Tensor\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "import time, datetime"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icBe4r3m-jqo"
      },
      "source": [
        "class UCRDataset(Dataset):\n",
        "    \"\"\"Dataset which samples the data from hourly electricity data.\"\"\"\n",
        "\n",
        "    def __init__(self, data, normalize=True, unsqueeze=True):\n",
        "        self.raw_data = data\n",
        "        Y, X = data[:, 0].astype(np.int32), data[:, 1:]\n",
        "        if normalize:\n",
        "            X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-8)\n",
        "\n",
        "        X = torch.Tensor(X)\n",
        "\n",
        "        if unsqueeze and len(X.shape) < 3:\n",
        "            X = X.unsqueeze(1)\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.raw_data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.X[idx, :]\n",
        "        label = int(self.Y[idx] == 1)\n",
        "        return sample, label\n",
        "\n",
        "    def __seqlen__(self):\n",
        "        return self.raw_data.shape[1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6xNjPu7W2uj"
      },
      "source": [
        "class ROCKET(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, seq_len, num_kernels=10000, kernel_sizes=[7, 9, 11], device=None):\n",
        "        super().__init__()\n",
        "        kernels = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_kernels):\n",
        "            kernel_size = np.random.choice(kernel_sizes)\n",
        "            dilation = 2 ** np.random.uniform(0, np.log2((seq_len - 1) // (kernel_size - 1)))\n",
        "            padding = int((kernel_size - 1) * dilation // 2) if np.random.randint(2) == 1 else 0\n",
        "\n",
        "            weight = torch.randn(1, in_channels, kernel_size)\n",
        "            weight -= weight.mean()\n",
        "            bias = 2 * torch.rand(1) - 1\n",
        "\n",
        "            kernel = nn.Conv1d(in_channels, 1, kernel_size, padding=2 * padding, dilation=int(dilation), bias=True)\n",
        "            kernel.weight = torch.nn.Parameter(weight, requires_grad=False)\n",
        "            kernel.bias = torch.nn.Parameter(bias, requires_grad=False)\n",
        "\n",
        "            kernels.append(kernel)\n",
        "\n",
        "        self.kernels = kernels\n",
        "        self.num_kernels = num_kernels\n",
        "        self.kss = kernel_sizes\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for i in range(self.num_kernels):\n",
        "            output = self.kernels[i](x).cpu()\n",
        "            kernel_max, _ = output.max(dim=-1)\n",
        "            kernel_ppv = torch.gt(output, 0).sum(dim=-1).float() / output.shape[-1]\n",
        "            features.append(kernel_max)\n",
        "            features.append(kernel_ppv)\n",
        "        output = torch.cat(features, dim=1)\n",
        "        return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91n9jlY2J0a-"
      },
      "source": [
        "def load_data(dataset_name):\n",
        "    train_data = np.loadtxt(f\"{input_path}/{dataset_name}/{dataset_name}_TRAIN.txt\")\n",
        "    test_data = np.loadtxt(f\"{input_path}/{dataset_name}/{dataset_name}_TEST.txt\")\n",
        "\n",
        "    train_ds = UCRDataset(train_data)\n",
        "    test_ds = UCRDataset(test_data)\n",
        "    \n",
        "    return train_ds, test_ds"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28Hlws73Aejl"
      },
      "source": [
        "dataset_names = ['FordA']\n",
        "num_runs = 1\n",
        "in_channels = 1\n",
        "num_kernels = 1000\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "results = pd.DataFrame(index=dataset_names,\n",
        "                       columns=[\"accuracy_mean\",\n",
        "                                \"accuracy_standard_deviation\",\n",
        "                                \"time_training_seconds\",\n",
        "                                \"time_test_seconds\"],\n",
        "                       data=0)\n",
        "results.index.name = \"dataset\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4LTvjL2W5L1",
        "outputId": "b94821b0-0aeb-4d1b-cafb-57a049354a91"
      },
      "source": [
        "for dataset_name in dataset_names:\n",
        "\n",
        "    print(f\"{dataset_name}\".center(80, \"-\"))\n",
        "\n",
        "    # -- read data -------------------------------------------------------------\n",
        "    train_ds, test_ds = load_data(dataset_name)\n",
        "    seq_len = train_ds.__seqlen__()\n",
        "\n",
        "    _results = np.zeros(num_runs)\n",
        "    _timings = np.zeros([4, num_runs])\n",
        "\n",
        "    X_train, Y_train = train_ds.X, train_ds.Y\n",
        "    X_test, Y_test = test_ds.X, test_ds.Y\n",
        "\n",
        "    X_train = X_train.to(device)\n",
        "    X_test = X_test.to(device)\n",
        "\n",
        "    # -- run -------------------------------------------------------------------\n",
        "\n",
        "    print(f\"Performing runs\".ljust(80 - 5, \".\"), end=\"\", flush=True)\n",
        "\n",
        "    for i in range(num_runs):\n",
        "\n",
        "        conv_model = ROCKET(in_channels, seq_len, num_kernels=num_kernels).to(device)\n",
        "\n",
        "        # -- transform training ------------------------------------------------\n",
        "        time_a = time.perf_counter()\n",
        "        X_train_transform = conv_model(X_train).cpu()\n",
        "        time_b = time.perf_counter()\n",
        "        _timings[0, i] = time_b - time_a\n",
        "        print(\"\\nconv1d transform train \" + str(time_b - time_a))\n",
        "\n",
        "        # -- transform test ----------------------------------------------------\n",
        "        time_a = time.perf_counter()\n",
        "        X_test_transform = conv_model(X_test).cpu()\n",
        "        time_b = time.perf_counter()\n",
        "        _timings[1, i] = time_b - time_a\n",
        "        print(\"conv1d transform test \" + str(time_b - time_a))\n",
        "\n",
        "        # -- training ----------------------------------------------------------\n",
        "        time_a = time.perf_counter()\n",
        "        classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
        "        classifier.fit(X_train_transform, Y_train)\n",
        "        time_b = time.perf_counter()\n",
        "        _timings[2, i] = time_b - time_a\n",
        "        print(\"training Rifge classifier \" + str(time_b - time_a))\n",
        "\n",
        "        # -- test --------------------------------------------------------------\n",
        "        time_a = time.perf_counter()\n",
        "        _results[i] = classifier.score(X_test_transform, Y_test)\n",
        "        time_b = time.perf_counter()\n",
        "        _timings[3, i] = time_b - time_a\n",
        "        print(\"Test evaluation \" + str(time_b - time_a))\n",
        "    print(\"Done.\")\n",
        "\n",
        "    results.loc[dataset_name, \"accuracy_mean\"] = _results.mean()\n",
        "    results.loc[dataset_name, \"accuracy_standard_deviation\"] = _results.std()\n",
        "    results.loc[dataset_name, \"time_training_seconds\"] = _timings.mean(1)[[0, 2]].sum()\n",
        "    results.loc[dataset_name, \"time_test_seconds\"] = _timings.mean(1)[[1, 3]].sum()\n",
        "\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(results)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------FordA--------------------------------------\n",
            "Performing runs............................................................\n",
            "conv1d transform train 10.465163385000324\n",
            "conv1d transform test 3.7843717979999383\n",
            "training Rifge classifier 9.12444040499986\n",
            "Test evaluation 0.008656824999889068\n",
            "Done.\n",
            "         accuracy_mean  accuracy_standard_deviation  time_training_seconds  \\\n",
            "dataset                                                                      \n",
            "FordA         0.937121                          0.0              19.589604   \n",
            "\n",
            "         time_test_seconds  \n",
            "dataset                     \n",
            "FordA             3.793029  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0oJFI8GMgiH",
        "outputId": "6d89f0c9-570b-43a1-c27a-25e92b205e0a"
      },
      "source": [
        "for dataset_name in dataset_names:\n",
        "\n",
        "    print(f\"{dataset_name}\".center(80, \"-\"))\n",
        "\n",
        "    # -- read data -------------------------------------------------------------\n",
        "    train_ds, test_ds = load_data(dataset_name)\n",
        "    seq_len = train_ds.__seqlen__()\n",
        "\n",
        "    _results = np.zeros(num_runs)\n",
        "    _timings = np.zeros([4, num_runs])\n",
        "        \n",
        "    batch_size = 64\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    def batch_transform(data_loader):\n",
        "        X_transform = []\n",
        "        Y = []\n",
        "        for _, (X, y) in enumerate(data_loader):\n",
        "            X_transform.append(conv_model(X.to(device)))\n",
        "            Y.append(y)\n",
        "        X_transform = torch.cat(X_transform, dim=0)\n",
        "        Y = torch.cat(Y, dim=0)\n",
        "        return X_transform, Y\n",
        "\n",
        "    for i in range(num_runs):\n",
        "        conv_model = ROCKET(in_channels, seq_len, num_kernels=num_kernels).to(device)\n",
        "\n",
        "        # -- transform training ------------------------------------------------\n",
        "        time_a = time.perf_counter()\n",
        "        X_train_transform, Y_train = batch_transform(train_loader)\n",
        "        time_b = time.perf_counter()\n",
        "        _timings[0, i] = time_b - time_a\n",
        "\n",
        "        # -- transform test ----------------------------------------------------\n",
        "        time_a = time.perf_counter()\n",
        "        X_test_transform, Y_test = batch_transform(test_loader)\n",
        "        time_b = time.perf_counter()\n",
        "        _timings[1, i] = time_b - time_a\n",
        "\n",
        "        # -- Normalizing Features ----------------------------------------------\n",
        "        f_mean = X_train_transform.mean(axis=0, keepdims=True)\n",
        "        f_std = X_train_transform.std(axis=0, keepdims=True) + 1e-6\n",
        "\n",
        "        X_train_transform = (X_train_transform - f_mean) / f_std\n",
        "        X_test_transform = (X_test_transform - f_mean) / f_std\n",
        "\n",
        "        # -- Dimensionality Reduction-------------------------------------------\n",
        "        from sklearn.decomposition import PCA\n",
        "        # num_features = min(2 * num_kernels, train_ds.__len__()) - 1\n",
        "        pca = PCA(n_components=0.99)\n",
        "        X_train_transform = torch.FloatTensor(pca.fit_transform(X_train_transform))\n",
        "        X_test_transform = torch.FloatTensor(pca.transform(X_test_transform))\n",
        "        num_features = pca.n_components_\n",
        "        print(\"num components=\" + str(num_features))\n",
        "\n",
        "        # -- training ----------------------------------------------------------\n",
        "\n",
        "        time_a = time.perf_counter()\n",
        "\n",
        "        train_features_ds = TensorDataset(X_train_transform, Y_train)\n",
        "        train_ft_loader = DataLoader(train_features_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        linear_model = nn.Linear(num_features, len(torch.unique(Y_train))).to(device)\n",
        "        loss_function = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(linear_model.parameters(), lr=5e-5, weight_decay=0.02)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, min_lr=1e-8)\n",
        "        \n",
        "        epochs = 250\n",
        "        print_every = 50\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Train Loop\n",
        "            linear_model.train()\n",
        "            for t, (X, y) in enumerate(train_ft_loader):\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_pred = linear_model(X)\n",
        "                loss = loss_function(y_pred, y)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Eval Loop\n",
        "            linear_model.eval()\n",
        "            y_pred = linear_model(X_test_transform.to(device)).to('cpu')\n",
        "            loss = loss_function(y_pred, Y_test)\n",
        "            acc = (y_pred.max(1)[1].numpy() == Y_test.numpy()).mean()\n",
        "            if epoch % print_every == 0:\n",
        "                print(str(datetime.datetime.now()),\n",
        "                      'Epoch=%d' % epoch,\n",
        "                      ' Valid. avg loss=%.4f' % loss.item(), \n",
        "                      ' Valid. avg acc=%.4f' % acc, \n",
        "                      'last_lr=', optimizer.param_groups[0]['lr'])\n",
        "            if scheduler:\n",
        "                scheduler.step(loss)\n",
        "        _results[i] = (y_pred.max(1)[1].numpy() == Y_test.numpy()).mean()\n",
        "        time_b = time.perf_counter()\n",
        "        _timings[2, i] = time_b - time_a\n",
        "\n",
        "    print(\"Done.\")\n",
        "    \n",
        "    results.loc[dataset_name, \"accuracy_mean\"] = _results.mean()\n",
        "    results.loc[dataset_name, \"accuracy_standard_deviation\"] = _results.std()\n",
        "    results.loc[dataset_name, \"time_training_seconds\"] = _timings.mean(1)[[0, 2]].sum()\n",
        "    results.loc[dataset_name, \"time_test_seconds\"] = _timings.mean(1)[[1, 3]].sum()\n",
        "\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(results)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------FordA--------------------------------------\n",
            "num components=1199\n",
            "2021-03-23 00:32:08.571966 Epoch=0  Valid. avg loss=0.8480  Valid. avg acc=0.4273 last_lr= 5e-05\n",
            "2021-03-23 00:32:12.789240 Epoch=50  Valid. avg loss=0.2112  Valid. avg acc=0.9402 last_lr= 5e-05\n",
            "2021-03-23 00:32:16.901908 Epoch=100  Valid. avg loss=0.1599  Valid. avg acc=0.9379 last_lr= 5e-05\n",
            "2021-03-23 00:32:21.081032 Epoch=150  Valid. avg loss=0.1534  Valid. avg acc=0.9379 last_lr= 5e-05\n",
            "2021-03-23 00:32:25.283047 Epoch=200  Valid. avg loss=0.1537  Valid. avg acc=0.9379 last_lr= 3.125e-06\n",
            "Done.\n",
            "         accuracy_mean  accuracy_standard_deviation  time_training_seconds  \\\n",
            "dataset                                                                      \n",
            "FordA         0.937879                          0.0              40.089094   \n",
            "\n",
            "         time_test_seconds  \n",
            "dataset                     \n",
            "FordA             7.015341  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KquDdWX5Tn4T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}